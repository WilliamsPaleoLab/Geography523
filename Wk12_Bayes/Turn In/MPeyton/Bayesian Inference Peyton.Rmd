---
title: "Bayesian Inference Assignment"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

###1. Coin flips

(this was completed during the class period)

###2. Markov chain Monte Carlo Sampling

Write a function to test the assertion that the proportion of visits to a state will converge to a proportion of the population

```{r}
library(datasets)

bayes.state <- function(n) {
  #First let's get the dataframe ready
state <- as.data.frame(state.x77)
state$visits <- seq(1,50,1)
state$visits <- state$visits*0
#Now set up the prior and posterior
x <- runif(1,1,50)
x.state <- rownames(state[x,])
x.pop <- state$Population[x]
state$visits[x] <- state$visits[x] + 1
y <- runif(1,1,50)
y.state <- rownames(state[y,])
y.pop <- state$Population[y]
#Let's loop
for (i in 1:(n-1)) {
if (y.pop/x.pop > 1) {x <- y; x.pop <- y.pop; x.state <- y.state
y <- runif(1,1,50); y.pop <- state$Population[y]; state$visits[x] <-
  state$visits[x] + 1}
else {ratio <- y.pop/x.pop; chance <- rbinom(1,1,ratio)}
  if (chance == 1) {x <- y; x.pop <- y.pop; x.state <- y.state
y <- runif(1,1,50); y.pop <- state$Population[y]; state$visits[x] <-
  state$visits[x] + 1}
  else {y <- runif(1,1,50); y.state <- rownames(state[y,]); y.pop <-
  state$Population[y]}
}  
output <- data.frame(State.name = rownames(state), state$Population, state$visits, Pop_Proportion = state$Population/sum(state$Population), Visit_Proportion = state$visits/sum(state$visits))
final.state <- data.frame(Final_State = x.state, Population = x.pop)
return(output)
}
#It's set up to return a dataframe with the porportions of populations and state visits. You can also set it up to return the final state by setting "return(final.state)" in the function. Just input 'n' to equal the number of visits.
```

###3. Compare Bayesian and Classical Linear Regressions

```{r, results = "hide"}
library(analogue)
library(palaeoSig)
library(MCMCpack)
data(arctic.pollen)
data(arctic.env)
```

```{r}
#Classic linear regression model
x.var <- arctic.env$tave
y.var <- x.var+rnorm(length(x.var), mean = 0, sd = 2)
classic.lm <- lm(y.var ~ x.var)

#MCMC Gaussian Linear Regression
mcmc.lm <- MCMCpack::MCMCregress(y.var ~ x.var)

#Let's compare the models
summary(classic.lm)
summary(mcmc.lm)
```

***Make predictions (first without including the variance component) for each of the posterior draws. Estimate uncertainties and compare to classic model**

```{r}
#We need to reduce the number of draws to a manageable number first
mcmc.lm <- MCMCpack::MCMCregress(y.var ~ x.var, mcmc = 100)
summary(mcmc.lm)
#Ok, now to predict all y values for each draw
mcmc.pred <- list()
for (i in 1:length(mcmc.lm[,1])) {mcmc.pred[[i]] <- 
  (mcmc.lm[i,1] + x.var*mcmc.lm[i,2])}
#Now to estimate uncertainties for predictions...

#Ok, I've tried multiple methods to isolate the values of the predictions for each model and group them together by the dependent variable so I can compare the uncertainties between all models for each point. Unfortunately, I don't seem to be able to quite get there with the code, and at this point I don't think I'll be able to do it on my own. Here are my closest two attempts, please let me know if you have any ideas as to how to make this work.
classic.pred <- predict(classic.lm, interval="confidence")
#1
mcmc.sd <- data.frame(1:length(mcmc.lm[,1]))
for (i in 1:length(mcmc.lm[,1])) {mcmc.sd[i,] <- 
  sd(mcmc.lm[i,1] + x.var*mcmc.lm[i,2])}
#2
mcmc.sd - (classic.pred[,3] - classic.pred[,2])
val <- data.frame(1:length(mcmc.pred))
for (i in 1:length(mcmc.lm[,1])) {for (x in 1:length(x.var)) {val[x] <- (mcmc.lm[[x,1]] + x.var[i]*mcmc.lm[[x,2]])}; mcmc.sd[i,] <- sd(val)}
```

**Make predictions including the variance component**

```{r}
mcmc.pred.e <- list()
for (i in 1:length(mcmc.lm[,1])) {mcmc.pred.e[[i]] <- 
  (mcmc.lm[i,1] + x.var*mcmc.lm[i,2] + rnorm(1, 0, sd=mcmc.lm[i,3]))}
#Let's plot the results
plot(mcmc.pred.e[[3]] ~ x.var)
```

Variance appears to be very small in the MCMC regression.
